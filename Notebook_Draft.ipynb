{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scope of Reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology\n",
    "\n",
    "Our GitHub repo is located [here](https://github.com/Eddie-Ward/cs598-DLH-team12-SAMIL).\n",
    "\n",
    "Please follow the setup instructions there including acquiring dataset access and setting up the Conda environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download Saved Checkpoints\n",
    "\n",
    "We provide the option to automatically download our saved checkpoints using `gdown`, or if that doesn't work, the Drive link to manually download our archived runs is [here](https://drive.google.com/drive/folders/1zy9JNd9pbQJhMTkI03AuhuU7Ntf7dpgc?usp=drive_link).\n",
    "\n",
    "If you are manually downloading, make sure to update the `ARCHIVED_RUNS_DIR` variable in the code below accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to automatically download our saved checkpoints with gdown\n",
    "import gdown\n",
    "\n",
    "ABMIL_folder_id = \"1XEKhFgUbxcPMRxhYgM0rxQSgmRxwnGqw\"\n",
    "gdown.download_folder(id=ABMIL_folder_id, output=\"./model_runs_gdown/ABMIL-52.5\")\n",
    "\n",
    "SAMIL_folder_id = \"1QBoMGcWC1zWu7cWqB06Zq6dgcL_11HQ_\"\n",
    "gdown.download_folder(id=SAMIL_folder_id, output=\"./model_runs_gdown/SAMIL-70.2\")\n",
    "\n",
    "SAMIL_imgcl_folder_id = \"1_4rQSxKpx079Giu1kggQGHU4zNhUrjbx\"\n",
    "gdown.download_folder(id=SAMIL_imgcl_folder_id, output=\"./model_runs_gdown/SAMIL-imgcl-70.3\")\n",
    "\n",
    "SAMIL_bag_pretrain_folder_id = \"1aE8Iv0c_nAD5g4sqBwkvGXWy0tMIINAH\"\n",
    "gdown.download_folder(id=SAMIL_bag_pretrain_folder_id, output=\"./model_runs_gdown/SAMIL-bag-pretrain-72.9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DOWNLOADED_RUNS_DIR = os.path.join(\"./model_runs_archived\") # Use this if you are manually downloading saved checkpoints, use this folder\n",
    "GDOWN_RUNS_DIR = os.path.join(\"./model_runs_gdown\") # Use this if you are running the gdown code to automatically download saved checkpoints\n",
    "\n",
    "ARCHIVED_RUNS_DIR = GDOWN_RUNS_DIR # Change between the two variables accordingly. ARCHIVED_RUNS_DIR is the global path variable referenced in the Results section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import glob\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify these if you have a different folder structure than the recommended one\n",
    "DATA_INFO_DIR = os.path.join(\"./data_info\")\n",
    "DATA_DIR = os.path.join(\"./echo_data\")\n",
    "CHECKPOINTS_DIR = os.path.join(\"./model_checkpoints\")\n",
    "RUNS_DIR = os.path.join(\"./model_runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"Cuda is available\")\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "\n",
    "    # May improve performance if GPU is Ampere architecture or later\n",
    "    # Comment these lines out if PyTorch fails\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "else:\n",
    "    print(\"Cuda is not available, using CPU\")\n",
    "    DEVICE = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the random seed to what is specified in original paper's repo\n",
    "seed = 0\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Source\n",
    "\n",
    "The dataset is from the TMED-2 dataset available here for approved users that apply on the website.\n",
    "https://tmed.cs.tufts.edu/tmed_v2.html\n",
    "\n",
    "This paper specifically uses the `view_and_diagnosis_labeled_set` although parts of the model are pretrained using other sets. \n",
    "\n",
    "Due to computation feasibility constraints, we are only using `Split 1` out of the 3 available DEV479 splits. DEV479 splits consist of 360 train studies, 119 val studies, and 120 test studies (called DEV479 because 360 + 119 = 479). These are pre-defined and specified in CSVs available from the original GitHub repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TMED2_SUMMARY_TABLE = pd.read_csv(os.path.join(DATA_INFO_DIR, \"TMED2SummaryTable.csv\"))\n",
    "\n",
    "SEED_DIR = os.path.join(DATA_INFO_DIR, \"seed0\") # Change this if using another seed (train/val/test split)\n",
    "\n",
    "TRAIN_SPLIT = pd.read_csv(os.path.join(SEED_DIR, \"train_studies.csv\"))\n",
    "VAL_SPLIT = pd.read_csv(os.path.join(SEED_DIR, \"val_studies.csv\"))\n",
    "TEST_SPLIT = pd.read_csv(os.path.join(SEED_DIR, \"test_studies.csv\"))\n",
    "\n",
    "train_study_ids = TRAIN_SPLIT[\"study\"].values\n",
    "val_study_ids = VAL_SPLIT[\"study\"].values\n",
    "test_study_ids = TEST_SPLIT[\"study\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity test for number of studies for each split\n",
    "\n",
    "assert len(train_study_ids) == 360\n",
    "assert len(val_study_ids) == 119\n",
    "assert len(test_study_ids) == 120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Dataset Class: EchoDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "class EchoDataset(Dataset):\n",
    "    def __init__(self, study_ids: pd.DataFrame, summary_table: pd.DataFrame, data_dir: str, transform_fn=None):\n",
    "        self.study_ids = study_ids\n",
    "        self.summary_table = summary_table\n",
    "        self.data_dir = data_dir\n",
    "        self.transform_fn = transform_fn\n",
    "        self.diagnosis_to_label = {\n",
    "            \"no_AS\": 0,\n",
    "            \"mild_AS\": 1,\n",
    "            \"mildtomod_AS\": 1,\n",
    "            \"moderate_AS\": 2,\n",
    "            \"severe_AS\": 2,\n",
    "        }\n",
    "\n",
    "        self.bag_of_study_images, self.bag_of_study_labels = self._create_bags()\n",
    "\n",
    "    def _create_bags(self):\n",
    "        bag_of_study_images = []\n",
    "        bag_of_study_labels = []\n",
    "\n",
    "        for study_id in self.study_ids:\n",
    "            summary_record = self.summary_table[self.summary_table[\"patient_study\"] == study_id]\n",
    "\n",
    "            label = list(set(summary_record[\"diagnosis_label\"].values))\n",
    "            assert len(label) == 1, \"every study should only have one diagnosis label\"\n",
    "\n",
    "            label = label[0]\n",
    "            assert label in self.diagnosis_to_label, \"every label should be valid\"\n",
    "\n",
    "            label = self.diagnosis_to_label[label]\n",
    "            pattern = study_id + \"_*.png\"\n",
    "\n",
    "            labeled_images_paths: list[str] = glob.glob(pathname=pattern, root_dir=os.path.join(self.data_dir, \"labeled\"))\n",
    "            unlabeled_images_paths: list[str] = glob.glob(\n",
    "                pathname=pattern, root_dir=os.path.join(self.data_dir, \"unlabeled\")\n",
    "            )\n",
    "\n",
    "            if len(labeled_images_paths) == 0 and len(unlabeled_images_paths) == 0:\n",
    "                print(f\"Couldn't find images for {study_id}\")\n",
    "                continue\n",
    "\n",
    "            labeled_images_paths.sort()\n",
    "            unlabeled_images_paths.sort()\n",
    "\n",
    "            study_images = []\n",
    "\n",
    "            for image_path in labeled_images_paths:\n",
    "                study_images.append(self._get_image_data(image_path, \"labeled\"))\n",
    "\n",
    "            for image_path in unlabeled_images_paths:\n",
    "                study_images.append(self._get_image_data(image_path, \"unlabeled\"))\n",
    "\n",
    "            study_images = np.array(study_images)\n",
    "\n",
    "            bag_of_study_images.append(study_images)    \n",
    "            bag_of_study_labels.append(label)\n",
    "\n",
    "        return bag_of_study_images, bag_of_study_labels\n",
    "\n",
    "    def _get_image_data(self, file_name: str, folder_name: str):\n",
    "        abs_image_path = os.path.join(self.data_dir, folder_name, file_name)\n",
    "        image_data = Image.open(abs_image_path)\n",
    "        image_arr = np.array(image_data.convert(mode=\"RGB\"))\n",
    "\n",
    "        assert image_arr.shape == (112, 112, 3), \"Images should all be 112 x 112 RGB\"\n",
    "        assert not np.any((image_arr < 0) | (image_arr > 255)), \"Images should not have negative or invalid RGB values\"\n",
    "\n",
    "        return image_arr\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.bag_of_study_labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        study_images = self.bag_of_study_images[index]\n",
    "        study_label = self.bag_of_study_labels[index]\n",
    "\n",
    "        if self.transform_fn is not None:\n",
    "            images_list = [\n",
    "                self.transform_fn(Image.fromarray(image, mode=\"RGB\")) for image in study_images\n",
    "            ]\n",
    "            study_images = torch.stack(images_list)\n",
    "\n",
    "        return study_images, study_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Processing: Transform Functions\n",
    "\n",
    "We are using the transform functions defined in the paper's original GitHub including an option to normalize images.\n",
    "For the standard train dataset, images will go through a `RandomCrop` and `RandomHorizontalFlip`. Based on the original Github, these transformations are used for ABMIL.\n",
    "\n",
    "For the `RandAug` train dataset, images will go through a random selection of transformations. Based on the original GitHub, this variation of the train dataset was used in the MoCo pretraining task and SAMIL runs. Because the original GitHub provides only saved checkpoints and does not provide implementation code for training the view classifier or for MoCo pretraining, we are also using `RandAug` for all SAMIL runs.    \n",
    "\n",
    "Note: The code has just been adapted to the `v2` module of `torchvision.transforms`. As such, soon-to-be deprecated methods like `transforms.toTensor()` are changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandAug transformation class\n",
    "\n",
    "from PIL import ImageOps, ImageEnhance, ImageDraw\n",
    "\n",
    "PARAMETER_MAX = 10\n",
    "\n",
    "def AutoContrast(img, **kwarg):\n",
    "    return ImageOps.autocontrast(img)\n",
    "\n",
    "\n",
    "def Brightness(img, v, max_v, bias=0):\n",
    "    v = _float_parameter(v, max_v) + bias\n",
    "    return ImageEnhance.Brightness(img).enhance(v)\n",
    "\n",
    "\n",
    "def Color(img, v, max_v, bias=0):\n",
    "    v = _float_parameter(v, max_v) + bias\n",
    "    return ImageEnhance.Color(img).enhance(v)\n",
    "\n",
    "\n",
    "def Contrast(img, v, max_v, bias=0):\n",
    "    v = _float_parameter(v, max_v) + bias\n",
    "    return ImageEnhance.Contrast(img).enhance(v)\n",
    "\n",
    "\n",
    "def Cutout(img, v, max_v, bias=0):\n",
    "    if v == 0:\n",
    "        return img\n",
    "    v = _float_parameter(v, max_v) + bias\n",
    "    v = int(v * min(img.size))\n",
    "    return CutoutAbs(img, v)\n",
    "\n",
    "\n",
    "def CutoutAbs(img, v, **kwarg):\n",
    "    w, h = img.size\n",
    "    x0 = np.random.uniform(0, w)\n",
    "    y0 = np.random.uniform(0, h)\n",
    "    x0 = int(max(0, x0 - v / 2.0))\n",
    "    y0 = int(max(0, y0 - v / 2.0))\n",
    "    x1 = int(min(w, x0 + v))\n",
    "    y1 = int(min(h, y0 + v))\n",
    "    xy = (x0, y0, x1, y1)\n",
    "    # gray\n",
    "    color = (127, 127, 127)\n",
    "    img = img.copy()\n",
    "    \n",
    "    ImageDraw.Draw(img).rectangle(xy, color)\n",
    "    return img\n",
    "\n",
    "\n",
    "def Equalize(img, **kwarg):\n",
    "    return ImageOps.equalize(img)\n",
    "\n",
    "\n",
    "def Identity(img, **kwarg):\n",
    "    return img\n",
    "\n",
    "\n",
    "def Invert(img, **kwarg):\n",
    "    return ImageOps.invert(img)\n",
    "\n",
    "\n",
    "def Posterize(img, v, max_v, bias=0):\n",
    "    v = _int_parameter(v, max_v) + bias\n",
    "    return ImageOps.posterize(img, v)\n",
    "\n",
    "\n",
    "def Rotate(img, v, max_v, bias=0):\n",
    "    v = _int_parameter(v, max_v) + bias\n",
    "    if random.random() < 0.5:\n",
    "        v = -v\n",
    "    return img.rotate(v)\n",
    "\n",
    "\n",
    "def Sharpness(img, v, max_v, bias=0):\n",
    "    v = _float_parameter(v, max_v) + bias\n",
    "    return ImageEnhance.Sharpness(img).enhance(v)\n",
    "\n",
    "\n",
    "def ShearX(img, v, max_v, bias=0):\n",
    "    v = _float_parameter(v, max_v) + bias\n",
    "    if random.random() < 0.5:\n",
    "        v = -v\n",
    "    return img.transform(img.size, PIL.Image.AFFINE, (1, v, 0, 0, 1, 0))\n",
    "\n",
    "\n",
    "def ShearY(img, v, max_v, bias=0):\n",
    "    v = _float_parameter(v, max_v) + bias\n",
    "    if random.random() < 0.5:\n",
    "        v = -v\n",
    "    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, 0, v, 1, 0))\n",
    "\n",
    "\n",
    "def Solarize(img, v, max_v, bias=0):\n",
    "    v = _int_parameter(v, max_v) + bias\n",
    "    return ImageOps.solarize(img, 256 - v)\n",
    "\n",
    "\n",
    "def SolarizeAdd(img, v, max_v, bias=0, threshold=128):\n",
    "    v = _int_parameter(v, max_v) + bias\n",
    "    if random.random() < 0.5:\n",
    "        v = -v\n",
    "    img_np = np.array(img).astype(np.int)\n",
    "    img_np = img_np + v\n",
    "    img_np = np.clip(img_np, 0, 255)\n",
    "    img_np = img_np.astype(np.uint8)\n",
    "    img = Image.fromarray(img_np)\n",
    "    return ImageOps.solarize(img, threshold)\n",
    "\n",
    "\n",
    "def TranslateX(img, v, max_v, bias=0):\n",
    "    v = _float_parameter(v, max_v) + bias\n",
    "    if random.random() < 0.5:\n",
    "        v = -v\n",
    "    v = int(v * img.size[0])\n",
    "    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, v, 0, 1, 0))\n",
    "\n",
    "\n",
    "def TranslateY(img, v, max_v, bias=0):\n",
    "    v = _float_parameter(v, max_v) + bias\n",
    "    if random.random() < 0.5:\n",
    "        v = -v\n",
    "    v = int(v * img.size[1])\n",
    "    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, 0, 0, 1, v))\n",
    "\n",
    "\n",
    "def _float_parameter(v, max_v):\n",
    "    return float(v) * max_v / PARAMETER_MAX\n",
    "\n",
    "\n",
    "def _int_parameter(v, max_v):\n",
    "    return int(v * max_v / PARAMETER_MAX)\n",
    "\n",
    "\n",
    "def fixmatch_augment_pool():\n",
    "    # FixMatch paper\n",
    "    augs = [\n",
    "        (AutoContrast, None, None),\n",
    "        (Brightness, 0.9, 0.05),\n",
    "        (Color, 0.9, 0.05),\n",
    "        (Contrast, 0.9, 0.05),\n",
    "        (Equalize, None, None),\n",
    "        (Identity, None, None),\n",
    "        (Posterize, 4, 4),\n",
    "        (Rotate, 30, 0),\n",
    "        (Sharpness, 0.9, 0.05),\n",
    "        (ShearX, 0.3, 0),\n",
    "        (ShearY, 0.3, 0),\n",
    "        (Solarize, 256, 0),\n",
    "        (TranslateX, 0.3, 0),\n",
    "        (TranslateY, 0.3, 0),\n",
    "    ]\n",
    "    return augs\n",
    "\n",
    "class RandAugmentMC(object):\n",
    "    def __init__(self, n, m):\n",
    "        assert n >= 1\n",
    "        assert 1 <= m <= 10\n",
    "        self.n = n\n",
    "        self.m = m\n",
    "        self.augment_pool = fixmatch_augment_pool()\n",
    "\n",
    "    def __call__(self, img):\n",
    "        ops = random.choices(self.augment_pool, k=self.n)\n",
    "        for op, max_v, bias in ops:\n",
    "            v = np.random.randint(1, self.m)\n",
    "            if random.random() < 0.5:\n",
    "                img = op(img, v=v, max_v=max_v, bias=bias)\n",
    "        img = CutoutAbs(img, int(32 * 0.5))\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform functions, can optionally choose to normalize datasets\n",
    "\n",
    "from torchvision.transforms import v2 as transforms\n",
    "\n",
    "normalized_mean = [0.059, 0.059, 0.059]\n",
    "normalized_std = [0.138, 0.138, 0.138]\n",
    "\n",
    "transform_train = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomCrop(\n",
    "            size=112, padding=int(112 * 0.125), padding_mode=\"reflect\"\n",
    "        ),\n",
    "        transforms.ToImage(),\n",
    "        transforms.ToDtype(torch.float32, scale=True),\n",
    "        # transforms.Normalize(mean=normalized_mean, std=normalized_std),\n",
    "    ]\n",
    ")\n",
    "\n",
    "transform_randaug_train = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomCrop(\n",
    "            size=112, padding=int(112 * 0.125), padding_mode=\"reflect\"\n",
    "        ),\n",
    "        RandAugmentMC(n=2, m=10),\n",
    "        transforms.ToImage(),\n",
    "        transforms.ToDtype(torch.float32, scale=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "transform_eval = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToImage(),\n",
    "        transforms.ToDtype(torch.float32, scale=True),\n",
    "        # transforms.Normalize(mean=normalized_mean, std=normalized_std),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = EchoDataset(train_study_ids, TMED2_SUMMARY_TABLE, DATA_DIR, transform_fn=transform_train)\n",
    "print(f\"Number of train studies: {len(train_dataset)}\")\n",
    "\n",
    "train_randaug_dataset = EchoDataset(train_study_ids, TMED2_SUMMARY_TABLE, DATA_DIR, transform_fn=transform_randaug_train)\n",
    "print(f\"Number of train (randaug) studies: {len(train_randaug_dataset)}\")\n",
    "\n",
    "val_dataset = EchoDataset(val_study_ids, TMED2_SUMMARY_TABLE, DATA_DIR, transform_fn=transform_eval)\n",
    "print(f\"Number of eval studies: {len(val_dataset)}\")\n",
    "\n",
    "test_dataset = EchoDataset(test_study_ids, TMED2_SUMMARY_TABLE, DATA_DIR, transform_fn=transform_eval)\n",
    "print(f\"Number of test studies: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Statistics\n",
    "\n",
    "Running the below code can provide some statistics such as counts for each diagnosis in the set and the mean number of images per study.\n",
    "Notably, the train dataset suffers from both fewer studies with no_AS as well as a smaller mean bag size (number of images per study) for those fewer studies, showing that the TMED-2 dataset is imbalanced.\n",
    "\n",
    "Additionally, there is code to preview some images from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Statistics\n",
    "code_to_diagnosis = [\"no_AS\", \"mod_AS\", \"sev_AS\"]\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=8)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=8)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=8)\n",
    "\n",
    "def get_stats(data_loader: DataLoader, mapping: list[str], dataset_name: str):\n",
    "    label_counts = [0, 0, 0]\n",
    "    bag_sizes = [[], [], []]\n",
    "    \n",
    "    for data, label in data_loader:\n",
    "        label_counts[label] += 1\n",
    "        bag_sizes[label].append(data.shape[1])\n",
    "    \n",
    "    bag_sizes = [np.array(bag_size) for bag_size in bag_sizes]\n",
    "    bag_sizes = [np.mean(bag_size) for bag_size in bag_sizes]\n",
    "\n",
    "    print(dataset_name)\n",
    "    \n",
    "    for i, diagnosis in enumerate(mapping):\n",
    "        print(f\"{diagnosis}\\t: count = {label_counts[i]},\\t mean bag size = {bag_sizes[i]}\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "get_stats(train_loader, code_to_diagnosis, \"Train Set\")\n",
    "get_stats(val_loader, code_to_diagnosis, \"Validation Set\")\n",
    "get_stats(test_loader, code_to_diagnosis, \"Test Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for previewing Dataset Images and Labels\n",
    "# Load the cell below to preview images from the datasets\n",
    "\n",
    "code_to_diagnosis = [\"no_AS\", \"mod_AS\", \"sev_AS\"]\n",
    "\n",
    "def imgshow(img, label_index, mapping):\n",
    "    np_img = img.numpy()\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(np.transpose(np_img, (1, 2, 0)))\n",
    "    plt.title(mapping[label_index])\n",
    "    plt.show()\n",
    "\n",
    "def show_study_images(iter_loader, k=8):\n",
    "    images, labels = next(iter_loader)\n",
    "    images = images.squeeze(0)\n",
    "    images = images[:k]\n",
    "\n",
    "    img = torchvision.utils.make_grid(images, padding=25)\n",
    "    imgshow(img, labels.item(), code_to_diagnosis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS CELL REPEATEDLY TO SEE DIFFERENT IMAGES\n",
    "# You can also switch among the different defined datasets to see the different transformations.\n",
    "# e.g. train_randug_dataset has different transformations from train_dataset\n",
    "\n",
    "# Unlike the actual test loader for evaluating models, this DataLoader has shuffle=True to preview different images each run\n",
    "display_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "iter_loader = iter(display_loader)\n",
    "for _ in range(10):\n",
    "    show_study_images(iter_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention-Based Multiple Instance Learning (ABMIL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(Attention, self).__init__()\n",
    "        self.L = 500\n",
    "        self.B = 250\n",
    "        self.D = 128\n",
    "        self.K = 1\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.feature_extractor_part1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 20, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Conv2d(20, 50, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            \n",
    "            # Below layers added in addition to original ABMIL paper\n",
    "            nn.Conv2d(50, 100, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Conv2d(100, 200, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "        )\n",
    "\n",
    "        self.feature_extractor_part2 = nn.Sequential(\n",
    "            nn.Linear(200 * 4 * 4, self.L),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.feature_extractor_part3 = nn.Sequential(\n",
    "            nn.Linear(self.L, self.B),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.B, self.L),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(self.L, self.D), \n",
    "            nn.Tanh(), \n",
    "            nn.Linear(self.D, self.K)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.L * self.K, self.num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.squeeze(0)\n",
    "\n",
    "        H = self.feature_extractor_part1(x)\n",
    "        H = H.view(-1, 200 * 4 * 4)\n",
    "        H = self.feature_extractor_part2(H)\n",
    "        H = self.feature_extractor_part3(H)\n",
    "\n",
    "        A = self.attention(H)\n",
    "        A = torch.transpose(A, 1, 0)\n",
    "        A = F.softmax(A, dim=1) # softmax over number of images\n",
    "\n",
    "        M = torch.mm(A, H) # M can be regarded as final representation of this bag\n",
    "\n",
    "        out = self.classifier(M) # Outputs logits, not softmax\n",
    "\n",
    "        return out, A  # A is the attention weights on each image of the bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity test for working model\n",
    "\n",
    "test_attention_model = Attention()\n",
    "input = torch.randn((1, 9, 3, 112, 112))\n",
    "\n",
    "num_images = input.shape[1]\n",
    "label, weights = test_attention_model(input)\n",
    "\n",
    "assert label.shape == (1, 3), \"Labels should match number of classes\"\n",
    "assert weights.shape == (1, num_images), \"Weights should match number of studies\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supervised Attention Multiple Instance Learning (SAMIL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### View Classifier\n",
    "\n",
    "The view classifier is based on the Wide ResNet architecture, specifically \"WRN-28-2\" that has a depth 28 and width 2.\n",
    "The below code is not modified from the paper's GitHub. \n",
    "\n",
    "As mentioned above, we will be loading a provided checkpoint for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_planes,\n",
    "        out_planes,\n",
    "        stride,\n",
    "        drop_rate=0.0,\n",
    "        activate_before_residual=False,\n",
    "    ):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes, momentum=0.001)\n",
    "        self.relu1 = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(out_planes, momentum=0.001)\n",
    "        self.relu2 = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            out_planes, out_planes, kernel_size=3, stride=1, padding=1, bias=False\n",
    "        )\n",
    "        self.drop_rate = drop_rate\n",
    "        self.equalInOut = in_planes == out_planes\n",
    "        self.convShortcut = (\n",
    "            (not self.equalInOut)\n",
    "            and nn.Conv2d(\n",
    "                in_planes,\n",
    "                out_planes,\n",
    "                kernel_size=1,\n",
    "                stride=stride,\n",
    "                padding=0,\n",
    "                bias=False,\n",
    "            )\n",
    "            or None\n",
    "        )\n",
    "        self.activate_before_residual = activate_before_residual\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.equalInOut and self.activate_before_residual == True:\n",
    "            x = self.relu1(self.bn1(x))\n",
    "        else:\n",
    "            out = self.relu1(self.bn1(x))\n",
    "        out = self.relu2(self.bn2(self.conv1(out if self.equalInOut else x)))\n",
    "        if self.drop_rate > 0:\n",
    "            out = F.dropout(out, p=self.drop_rate, training=self.training)\n",
    "        out = self.conv2(out)\n",
    "        return torch.add(x if self.equalInOut else self.convShortcut(x), out)\n",
    "\n",
    "\n",
    "class NetworkBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        nb_layers,\n",
    "        in_planes,\n",
    "        out_planes,\n",
    "        block,\n",
    "        stride,\n",
    "        drop_rate=0.0,\n",
    "        activate_before_residual=False,\n",
    "    ):\n",
    "        super(NetworkBlock, self).__init__()\n",
    "        self.layer = self._make_layer(\n",
    "            block,\n",
    "            in_planes,\n",
    "            out_planes,\n",
    "            nb_layers,\n",
    "            stride,\n",
    "            drop_rate,\n",
    "            activate_before_residual,\n",
    "        )\n",
    "\n",
    "    def _make_layer(\n",
    "        self,\n",
    "        block,\n",
    "        in_planes,\n",
    "        out_planes,\n",
    "        nb_layers,\n",
    "        stride,\n",
    "        drop_rate,\n",
    "        activate_before_residual,\n",
    "    ):\n",
    "        layers = []\n",
    "        for i in range(int(nb_layers)):\n",
    "            layers.append(\n",
    "                block(\n",
    "                    i == 0 and in_planes or out_planes,\n",
    "                    out_planes,\n",
    "                    i == 0 and stride or 1,\n",
    "                    drop_rate,\n",
    "                    activate_before_residual,\n",
    "                )\n",
    "            )\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "\n",
    "class WideResNet(nn.Module):\n",
    "    def __init__(self, num_classes, depth=28, widen_factor=2, drop_rate=0.0):\n",
    "        super(WideResNet, self).__init__()\n",
    "        channels = [\n",
    "            16,\n",
    "            16 * widen_factor,\n",
    "            32 * widen_factor,\n",
    "            64 * widen_factor,\n",
    "            128 * widen_factor,\n",
    "        ]\n",
    "        assert (depth - 4) % 6 == 0\n",
    "        n = (depth - 4) / 6  # equivalent to 'repeat' in tf repo\n",
    "        block = BasicBlock\n",
    "        # 1st conv before any network block\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            3, channels[0], kernel_size=3, stride=1, padding=1, bias=False\n",
    "        )\n",
    "        # 1st block\n",
    "        self.block1 = NetworkBlock(\n",
    "            n,\n",
    "            channels[0],\n",
    "            channels[1],\n",
    "            block,\n",
    "            1,\n",
    "            drop_rate,\n",
    "            activate_before_residual=True,\n",
    "        )\n",
    "        # 2nd block\n",
    "        self.block2 = NetworkBlock(n, channels[1], channels[2], block, 2, drop_rate)\n",
    "        # 3rd block\n",
    "        self.block3 = NetworkBlock(n, channels[2], channels[3], block, 2, drop_rate)\n",
    "\n",
    "        # 4th block (hz added)\n",
    "        self.block4 = NetworkBlock(n, channels[3], channels[4], block, 2, drop_rate)\n",
    "\n",
    "        # global average pooling and classifier\n",
    "        self.bn1 = nn.BatchNorm2d(channels[4], momentum=0.001)\n",
    "        self.relu = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
    "        self.fc = nn.Linear(channels[4], num_classes)\n",
    "        self.channels = channels[4]\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(\n",
    "                    m.weight, mode=\"fan_out\", nonlinearity=\"leaky_relu\"\n",
    "                )\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1.0)\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.block1(out)\n",
    "        out = self.block2(out)\n",
    "        out = self.block3(out)\n",
    "        out = self.block4(out)\n",
    "\n",
    "        out = self.relu(self.bn1(out))\n",
    "        out = F.adaptive_avg_pool2d(out, 1)\n",
    "        out = out.view(-1, self.channels)\n",
    "        return self.fc(out)\n",
    "\n",
    "\n",
    "def build_wideresnet(depth, widen_factor, dropout, num_classes):\n",
    "    return WideResNet(\n",
    "        depth=depth,\n",
    "        widen_factor=widen_factor,\n",
    "        drop_rate=dropout,\n",
    "        num_classes=num_classes,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SAMIL Architecture (Similar to ABMIL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAMIL(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(SAMIL, self).__init__()\n",
    "        self.L = 500\n",
    "        self.B = 250\n",
    "        self.D = 128\n",
    "        self.K = 1\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.feature_extractor_part1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 20, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Conv2d(20, 50, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            \n",
    "            # Below layers added in addition to original ABMIL paper\n",
    "            nn.Conv2d(50, 100, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Conv2d(100, 200, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "        )\n",
    "\n",
    "        self.feature_extractor_part2 = nn.Sequential(\n",
    "            nn.Linear(200 * 4 * 4, self.L),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.feature_extractor_part3 = nn.Sequential(\n",
    "            nn.Linear(self.L, self.B),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.B, self.L),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.attention_V = nn.Sequential(\n",
    "            nn.Linear(self.L, self.D), \n",
    "            nn.Tanh(), \n",
    "            nn.Linear(self.D, self.K)\n",
    "        )\n",
    "\n",
    "        self.attention_U = nn.Sequential(\n",
    "            nn.Linear(self.L, self.D), \n",
    "            nn.Tanh(), \n",
    "            nn.Linear(self.D, self.K)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.L * self.K, self.num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.squeeze(0)\n",
    "\n",
    "        H = self.feature_extractor_part1(x)\n",
    "        H = H.view(-1, 200 * 4 * 4)\n",
    "        H = self.feature_extractor_part2(H)\n",
    "\n",
    "        A_V = self.attention_V(H)  # NxK\n",
    "        A_V = torch.transpose(A_V, 1, 0)  # KxN\n",
    "        A_V = F.softmax(A_V, dim=1)  # softmax over number of images\n",
    "\n",
    "        H = self.feature_extractor_part3(H)\n",
    "\n",
    "        A_U = self.attention_U(H)  # NxK\n",
    "        A_U = torch.transpose(A_U, 1, 0)  # KxN\n",
    "        A_U = F.softmax(A_U, dim=1)  # softmax over number of images\n",
    "\n",
    "        A = torch.exp(torch.log(A_V) + torch.log(A_U))  # numerically more stable softmax implementation\n",
    "        A = A / torch.sum(A)\n",
    "\n",
    "        M = torch.mm(A, H) # KxL, M can be regarded as final representation of this bag\n",
    "\n",
    "        out = self.classifier(M) # Outputs logits and not softmax\n",
    "\n",
    "        return out, A_V  # Only view regularize one branch of the attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity test for working SAMIL model\n",
    "\n",
    "test_samil_model = SAMIL()\n",
    "rgb_input = torch.randn((1, 9, 3, 112, 112))\n",
    "\n",
    "num_images = rgb_input.shape[1]\n",
    "label, weights = test_samil_model(rgb_input)\n",
    "\n",
    "assert label.shape == (1, 3), \"Labels should match number of classes\"\n",
    "assert weights.shape == (1, num_images), \"Weights should match number of studies\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cosine Scheduler with Warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    lr_warmup_epochs,\n",
    "    lr_cycle_epochs,  # total train epochs\n",
    "    num_cycles=7.0 / 16.0,\n",
    "    last_epoch=-1,\n",
    "):\n",
    "    def _lr_lambda(current_epoch):\n",
    "        if current_epoch < lr_warmup_epochs:\n",
    "            return float(current_epoch) / float(max(1, lr_warmup_epochs))\n",
    "\n",
    "        # see if using restart\n",
    "        ###############################################################\n",
    "        if current_epoch % lr_cycle_epochs == 0:\n",
    "            current_cycle_epoch = lr_cycle_epochs\n",
    "        else:\n",
    "            current_cycle_epoch = current_epoch % lr_cycle_epochs\n",
    "\n",
    "        no_progress = float(current_cycle_epoch - lr_warmup_epochs) / float(\n",
    "            max(1, float(lr_cycle_epochs) - lr_warmup_epochs)\n",
    "        )\n",
    "        #################################################################\n",
    "\n",
    "        return max(0.0, math.cos(math.pi * num_cycles * no_progress))\n",
    "\n",
    "    return optim.lr_scheduler.LambdaLR(optimizer, _lr_lambda, last_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Saving Model Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_checkpoint(model: Attention | SAMIL, filepath: str):\n",
    "    torch.save(model.state_dict(), filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Eval Function: Balanced Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "def eval_model(model: Attention | SAMIL, dataloader: DataLoader):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ground_truth_labels = []\n",
    "        pred_labels = []\n",
    "\n",
    "        for data, bag_label in dataloader:\n",
    "            data, bag_label = data.to(DEVICE), bag_label.to(DEVICE)\n",
    "\n",
    "            pred_logit, _ = model(data)\n",
    "\n",
    "            pred_label = torch.softmax(pred_logit, dim=-1)\n",
    "            pred_label = torch.argmax(pred_label).item()\n",
    "\n",
    "            pred_labels.append(pred_label)\n",
    "            ground_truth_labels.append(bag_label.item())\n",
    "\n",
    "        bal_acc = balanced_accuracy_score(ground_truth_labels, pred_labels)\n",
    "\n",
    "    return bal_acc\n",
    "\n",
    "def eval_model_with_ema(model: Attention | SAMIL, dataloader: DataLoader, ema_model):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ground_truth_labels = []\n",
    "        pred_labels = []\n",
    "        ema_pred_labels = []\n",
    "\n",
    "        for data, bag_label in dataloader:\n",
    "            data, bag_label = data.to(DEVICE), bag_label.to(DEVICE)\n",
    "\n",
    "            pred_logit, _ = model(data)\n",
    "            ema_pred_logit, _ = ema_model(data)\n",
    "\n",
    "            pred_label = torch.softmax(pred_logit, dim=-1)\n",
    "            pred_label = torch.argmax(pred_label).detach().cpu().item()\n",
    "\n",
    "            ema_pred_label = torch.softmax(ema_pred_logit, dim=-1)\n",
    "            ema_pred_label = torch.argmax(ema_pred_label).detach().cpu().item()\n",
    "\n",
    "            pred_labels.append(pred_label)\n",
    "            ema_pred_labels.append(ema_pred_label)\n",
    "            ground_truth_labels.append(bag_label.detach().cpu().item())\n",
    "\n",
    "        bal_acc = balanced_accuracy_score(ground_truth_labels, pred_labels)\n",
    "        ema_bal_acc = balanced_accuracy_score(ground_truth_labels, ema_pred_labels)\n",
    "\n",
    "    return bal_acc, ema_bal_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training ABMIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ABMIL_one_epoch(\n",
    "    model: Attention, \n",
    "    train_loader: DataLoader, \n",
    "    optimizer: optim.SGD | optim.Adam,\n",
    "    class_weights: torch.Tensor,\n",
    "):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for data, label in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        data, label = data.to(DEVICE), label.to(DEVICE)\n",
    "\n",
    "        pred_labels, _ = model(data)\n",
    "        pred_labels = pred_labels.to(DEVICE)\n",
    "\n",
    "        loss = F.cross_entropy(pred_labels, label, weight=class_weights)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.detach().cpu().item()\n",
    "\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ABMIL_model(\n",
    "    model: Attention,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    test_loader: DataLoader,\n",
    "    optimizer: optim.SGD | optim.Adam,\n",
    "    class_weights: torch.Tensor,\n",
    "    train_csv_path: str,\n",
    "    val_csv_path: str,\n",
    "    checkpoint_save_path: str,\n",
    "    best_checkpoint_save_path: str,\n",
    "    num_epochs: int,\n",
    "    eval_freq_epochs: int,\n",
    "    scheduler=None,\n",
    "    patience=200,\n",
    "    early_stop_warmup=500,\n",
    "):\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    train_df = pd.DataFrame(index=range(0, num_epochs), columns=[\"loss\"], dtype=\"Float32\")\n",
    "    val_df = pd.DataFrame(index=range(0, num_epochs, eval_freq_epochs), columns=[\"val_bal_acc\", \"test_bal_acc\"], dtype=\"Float32\")\n",
    "\n",
    "    best_val_bal_acc = 0\n",
    "    best_test_bal_acc = 0\n",
    "    early_stop_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_ABMIL_one_epoch(model, train_loader, optimizer, class_weights)\n",
    "        train_df.loc[epoch] = train_loss\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        if epoch % eval_freq_epochs == 0:\n",
    "            val_bal_acc = eval_model(model, val_loader)\n",
    "            val_df.loc[[epoch], [\"val_bal_acc\"]] = val_bal_acc\n",
    "\n",
    "            if val_bal_acc > best_val_bal_acc:\n",
    "                best_val_bal_acc = val_bal_acc\n",
    "                early_stop_counter = 0\n",
    "\n",
    "                test_bal_acc = eval_model(model, test_loader)\n",
    "                val_df.loc[[epoch], [\"test_bal_acc\"]] = test_bal_acc\n",
    "\n",
    "                save_model_checkpoint(model, checkpoint_save_path)\n",
    "\n",
    "                if test_bal_acc > best_test_bal_acc:\n",
    "                    best_test_bal_acc = test_bal_acc\n",
    "                    save_model_checkpoint(model, best_checkpoint_save_path)\n",
    "\n",
    "            else:\n",
    "                if epoch > early_stop_warmup:\n",
    "                    early_stop_counter += 1\n",
    "                    if early_stop_counter > patience:\n",
    "                        print(\n",
    "                            f\"Val bal acc has not improved for {patience} cycles, stopping early.\"\n",
    "                        )\n",
    "                        break\n",
    "\n",
    "            print(f\"Epoch: {epoch}\\t Training Loss: {train_loss:.6f}\\t Eval_Bal_Acc: {val_bal_acc}\")\n",
    "\n",
    "    train_df.to_csv(train_csv_path, sep=\",\", header=True, index=True, index_label=\"epoch\")\n",
    "    val_df.to_csv(val_csv_path, sep=\",\", header=True, index=True, index_label=\"epoch\")\n",
    "\n",
    "    print(f\"Best test bal acc: {best_test_bal_acc}, Best val bal acc: {best_val_bal_acc}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the main training loop. Do not run the cell if you do not want to train the model. Alternatively, ensure that `num_epochs` is something low like 2 if you want to verify the code works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters from original paper\n",
    "class_weights = torch.tensor([0.463, 0.342, 0.195]).to(DEVICE) # Class weights for CrossEntropyLoss from original GitHub\n",
    "learning_rate = 0.0008 # Learning rate from Appendix Table C.3. ABMIL Split 1\n",
    "weight_decay = 0.0001 # Weight decay from Appendix Table C.3 ABMIL Split 1\n",
    "num_epochs = 2000 # All training is done with 2000 epochs with early stop from Appendix C\n",
    "# Also using cosine schedule for all training from Appendix C\n",
    "\n",
    "eval_freq_epochs = 1 # Frequency of evaluating model's balanced accuracy on val set\n",
    "\n",
    "train_csv = os.path.join(RUNS_DIR, \"ABMIL\", \"train_last_checkpoint.csv\")\n",
    "val_csv = os.path.join(RUNS_DIR, \"ABMIL\", \"val_last_checkpoint.csv\")\n",
    "checkpoint_filepath = os.path.join(RUNS_DIR, \"ABMIL\", \"best_checkpoint.pth.tar\")\n",
    "best_checkpoint_filepath = os.path.join(RUNS_DIR, \"ABMIL\", \"best_test_checkpoint.pth.tar\")\n",
    "last_checkpoint_filepath = os.path.join(RUNS_DIR, \"ABMIL\", \"last_checkpoint.pth.tar\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=8)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=8)\n",
    "ABMIL_model = Attention().to(DEVICE)\n",
    "\n",
    "no_decay = {\"bias\", \"bn\"}\n",
    "\n",
    "grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [\n",
    "            p\n",
    "            for n, p in ABMIL_model.named_parameters()\n",
    "            if not any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        \"weight_decay\": weight_decay,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [\n",
    "            p for n, p in ABMIL_model.named_parameters() if any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "\n",
    "optimizer = optim.SGD(grouped_parameters, lr=learning_rate, momentum=0.9, nesterov=True) # Momentum and using Nesterov from original GitHub\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, 0, num_epochs) # Cosine schedule\n",
    "\n",
    "ABMIL_model = train_ABMIL_model(\n",
    "    ABMIL_model, \n",
    "    train_loader, \n",
    "    val_loader,\n",
    "    test_loader, \n",
    "    optimizer, \n",
    "    class_weights, \n",
    "    train_csv, \n",
    "    val_csv,\n",
    "    checkpoint_filepath,\n",
    "    best_checkpoint_filepath, \n",
    "    num_epochs, \n",
    "    eval_freq_epochs,\n",
    "    scheduler=scheduler, \n",
    ")\n",
    "\n",
    "save_model_checkpoint(ABMIL_model, last_checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training SAMIL\n",
    "\n",
    "We are using a provided checkpoint for the view classifier available on the original paper's Github.\n",
    "\n",
    "SAMIL training introduces two new hyperparamters: `temperature` (T) and `lambda_view_regularization` ($\\lambda_{SA}$).\n",
    "\n",
    "Temperature is used to scale the view relevance probabilities from the view classifier in the softmax transform.\n",
    "View relevance probability is defined as the sum of the probabilitiy that the specific image is PLAX or PSAX.\n",
    "Note that PLAX or PSAX are the only views that show the aortic valve and are relevant to AS diagnosis.\n",
    "\n",
    "Lambda_view_regualarization is a scalar value used to balance the loss function between cross-entropy loss (which is the only loss function in ABMIL) and supervised attention loss.\n",
    "Specifically, supervised attention loss is calculated from the KL divergence between view relevance scores obtained from the view classifier and the attention weights produced in SAMIL.\n",
    "\n",
    "The total loss function for SAMIL now accounts for the original CELoss from ABMIL as well as a supervised attention loss from the KL divergence between SAMIL's attention weights and the view relevance classifier weights.\n",
    "$$\n",
    "    \\mathcal{L} = \\mathcal{L}_{CE} + \\lambda_{SA}\\mathcal{L}_{SA}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_view_model(pretrained=True):\n",
    "    view_model = build_wideresnet(\n",
    "        depth=28, widen_factor=2, dropout=0.0, num_classes=3\n",
    "    )\n",
    "\n",
    "    # load the saved checkpoint\n",
    "    if pretrained:\n",
    "        view_checkpoint_path = os.path.join(\n",
    "            CHECKPOINTS_DIR, \"view_classifier\", \"seed0_model_best.pth.tar\"\n",
    "        )\n",
    "        view_checkpoint = torch.load(view_checkpoint_path)\n",
    "        view_model.load_state_dict(view_checkpoint[\"ema_state_dict\"])\n",
    "        view_model.eval()\n",
    "\n",
    "    return view_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity test for View Model\n",
    "temperature = 0.1 # Hyperparameter for training SAMIL, explained below\n",
    "\n",
    "test_view_model = create_view_model()\n",
    "\n",
    "rgb_input = torch.randn((1, 9, 3, 112, 112))\n",
    "\n",
    "num_images = rgb_input.shape[1]\n",
    "\n",
    "with torch.no_grad():\n",
    "    view_predictions = test_view_model(rgb_input.squeeze(0))\n",
    "    softmax_view_predictions = F.softmax(view_predictions, dim=1)\n",
    "\n",
    "    # The first two indices indicate probabilities of image being a PLAX or PSAX view\n",
    "    predicted_relevance = softmax_view_predictions[:, :2] \n",
    "\n",
    "    # Summing the two generates a probability of the view being relevant to diagnosing AS\n",
    "    predicted_relevance = torch.sum(predicted_relevance, dim=1)\n",
    "\n",
    "    predicted_relative_relevance = F.softmax(predicted_relevance / temperature, dim=-1)\n",
    "    predicted_relative_relevance = predicted_relative_relevance.unsqueeze(0)\n",
    "\n",
    "assert predicted_relative_relevance.shape == (1, num_images), \"Weights should match number of studies\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_SAMIL_one_epoch(\n",
    "    model: Attention,\n",
    "    view_model: WideResNet,\n",
    "    train_loader: DataLoader,\n",
    "    optimizer: optim.SGD | optim.Adam,\n",
    "    class_weights: torch.Tensor,\n",
    "    temperature: float,\n",
    "    lambda_view_regularization: float,\n",
    "    cur_epoch: int,\n",
    "    view_regularization_warmup_pos: float,\n",
    "):\n",
    "    model.train()\n",
    "    view_model.eval()\n",
    "    train_loss = 0\n",
    "\n",
    "    for data, label in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        data, label = data.to(DEVICE), label.to(DEVICE)\n",
    "\n",
    "        pred_label, attentions = model(data)\n",
    "        pred_label, attentions = pred_label.to(DEVICE), attentions.to(DEVICE)\n",
    "\n",
    "        log_attentions = torch.log(attentions)\n",
    "\n",
    "        # Get predicted view relevance for each image from pretrained view classifier\n",
    "        with torch.no_grad():\n",
    "            view_predictions = view_model(data.squeeze(0))\n",
    "            view_predictions = view_predictions.to(DEVICE)\n",
    "\n",
    "            softmax_view_predictions = F.softmax(view_predictions, dim=1)\n",
    "\n",
    "            predicted_relevance = softmax_view_predictions[:, :2]\n",
    "            predicted_relevance = torch.sum(predicted_relevance, dim=1)\n",
    "\n",
    "            predicted_relative_relevance = F.log_softmax(\n",
    "                predicted_relevance / temperature, dim=-1\n",
    "            )\n",
    "            predicted_relative_relevance = predicted_relative_relevance.unsqueeze(0)\n",
    "\n",
    "        CE_Loss = F.cross_entropy(pred_label, label, class_weights)\n",
    "        ViewRegularization_Loss = F.kl_div(\n",
    "            input=log_attentions,\n",
    "            target=predicted_relative_relevance,\n",
    "            log_target=True,\n",
    "            reduction=\"batchmean\",\n",
    "        )\n",
    "\n",
    "        current_warmup = np.clip(\n",
    "            cur_epoch / (float(view_regularization_warmup_pos) * num_epochs),\n",
    "            0,\n",
    "            1,\n",
    "        )\n",
    "\n",
    "        total_loss = (\n",
    "            CE_Loss\n",
    "            + lambda_view_regularization * ViewRegularization_Loss * current_warmup\n",
    "        )\n",
    "\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += total_loss.detach().cpu().item()\n",
    "\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_regularization_warmup_pos = 0.4\n",
    "\n",
    "def train_SAMIL_model(\n",
    "    model: SAMIL,\n",
    "    view_model: WideResNet,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    test_loader: DataLoader,\n",
    "    optimizer: optim.SGD | optim.Adam,\n",
    "    train_csv_path: str,\n",
    "    val_csv_path: str,\n",
    "    checkpoint_save_path: str,\n",
    "    best_checkpoint_save_path: str,\n",
    "    class_weights: torch.Tensor,\n",
    "    num_epochs: int,\n",
    "    eval_freq_epochs: int,\n",
    "    temperature: float,\n",
    "    lambda_view_regularization: float,\n",
    "    scheduler=None,\n",
    "    patience=200,\n",
    "    early_stop_warmup=200,\n",
    "):\n",
    "    model.to(DEVICE)\n",
    "    view_model.to(DEVICE)\n",
    "\n",
    "    train_df = pd.DataFrame(\n",
    "        index=range(0, num_epochs), columns=[\"loss\"], dtype=\"Float32\"\n",
    "    )\n",
    "    val_df = pd.DataFrame(\n",
    "        index=range(0, num_epochs, eval_freq_epochs),\n",
    "        columns=[\"val_bal_acc\", \"test_bal_acc\"],\n",
    "        dtype=\"Float32\",\n",
    "    )\n",
    "\n",
    "    best_val_bal_acc = 0\n",
    "    best_test_bal_acc = 0\n",
    "    early_stop_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_SAMIL_one_epoch(model, view_model, train_loader, optimizer, class_weights, temperature, lambda_view_regularization, epoch, view_regularization_warmup_pos)\n",
    "        train_df.loc[epoch] = train_loss\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        if epoch % eval_freq_epochs == 0:\n",
    "            val_bal_acc = eval_model(model, val_loader)\n",
    "            val_df.loc[[epoch], [\"val_bal_acc\"]] = val_bal_acc\n",
    "\n",
    "            if val_bal_acc > best_val_bal_acc:\n",
    "                best_val_bal_acc = val_bal_acc\n",
    "                early_stop_counter = 0\n",
    "\n",
    "                test_bal_acc = eval_model(model, test_loader)\n",
    "                val_df.loc[[epoch], [\"test_bal_acc\"]] = test_bal_acc\n",
    "\n",
    "                save_model_checkpoint(model, checkpoint_save_path)\n",
    "\n",
    "                if test_bal_acc > best_test_bal_acc:\n",
    "                    best_test_bal_acc = test_bal_acc\n",
    "                    save_model_checkpoint(model, best_checkpoint_save_path)\n",
    "            \n",
    "            else:\n",
    "                if epoch > early_stop_warmup:\n",
    "                    early_stop_counter += 1\n",
    "                    if early_stop_counter > patience:\n",
    "                        print(f\"Val bal acc has not improved for {patience} cycles, stopping early.\")\n",
    "                        break\n",
    "\n",
    "            print(f\"Epoch: {epoch}\\t Training Loss: {train_loss:.6f}\\t Eval_Bal_Acc: {val_bal_acc}\")\n",
    "\n",
    "    train_df.to_csv(train_csv_path, sep=\",\", header=True, index=True, index_label=\"epoch\")\n",
    "    val_df.to_csv(val_csv_path, sep=\",\", header=True, index=True, index_label=\"epoch\")\n",
    "\n",
    "    print(f\"Best val bal acc: {best_val_bal_acc}, Best test bal acc: {best_test_bal_acc}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training SAMIL - No Pretrain\n",
    "\n",
    "Below is the main training loop. Do not run the cell if you do not want to train the model. Alternatively, ensure that `num_epochs` is something low like 2 if you want to verify the code works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters from original paper\n",
    "class_weights = torch.tensor([0.463, 0.342, 0.195]).to(\n",
    "    torch.device(\"cuda\")\n",
    ")  # Class weights for CrossEntropyLoss from original GitHub\n",
    "learning_rate = 0.0005  # Learning rate from Appendix Table C.3. ABMIL Split 1\n",
    "weight_decay = 0.0001  # Weight decay from Appendix Table C.3 ABMIL Split 1\n",
    "\n",
    "temperature = 0.1  # Temperature from Appendix Table C.1 SAMIL Split 1\n",
    "lambda_view_regularization = 15.0  # Lambda_SA from Appendix Table C.1 SAMIL Split 1\n",
    "\n",
    "num_epochs = 2000  # All training is done with 2000 epochs with early stop from Appendix C\n",
    "# Also using cosine schedule for all training from Appendix C\n",
    "\n",
    "eval_freq_epochs = 1  # Frequency of evaluating model's balanced accuracy on val set\n",
    "\n",
    "train_csv = os.path.join(RUNS_DIR, \"SAMIL\", \"train_last_checkpoint.csv\")\n",
    "val_csv = os.path.join(RUNS_DIR, \"SAMIL\", \"val_last_checkpoint.csv\")\n",
    "checkpoint_filepath = os.path.join(RUNS_DIR, \"SAMIL\", \"best_checkpoint.pth.tar\")\n",
    "best_checkpoint_filepath = os.path.join(RUNS_DIR, \"SAMIL\", \"best_test_checkpoint.pth.tar\")\n",
    "last_checkpoint_filepath = os.path.join(RUNS_DIR, \"SAMIL\", \"last_checkpoint.pth.tar\")\n",
    "\n",
    "train_loader = DataLoader(train_randaug_dataset, batch_size=1, shuffle=True, num_workers=8)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=8)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=8)\n",
    "SAMIL_model = SAMIL().to(DEVICE)\n",
    "view_model = create_view_model().to(DEVICE)\n",
    "\n",
    "no_decay = {\"bias\", \"bn\"}\n",
    "\n",
    "grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [\n",
    "            p\n",
    "            for n, p in SAMIL_model.named_parameters()\n",
    "            if not any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        \"weight_decay\": weight_decay,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [\n",
    "            p\n",
    "            for n, p in SAMIL_model.named_parameters()\n",
    "            if any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "\n",
    "optimizer = optim.SGD(grouped_parameters, lr=learning_rate, momentum=0.9, nesterov=True)\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, 0, num_epochs)\n",
    "\n",
    "SAMIL_model = train_SAMIL_model(\n",
    "    SAMIL_model,\n",
    "    view_model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    test_loader,\n",
    "    optimizer,\n",
    "    train_csv,\n",
    "    val_csv,\n",
    "    checkpoint_filepath,\n",
    "    best_checkpoint_filepath,\n",
    "    class_weights,\n",
    "    num_epochs,\n",
    "    eval_freq_epochs,\n",
    "    temperature,\n",
    "    lambda_view_regularization,\n",
    "    scheduler=scheduler,\n",
    ")\n",
    "\n",
    "save_model_checkpoint(SAMIL_model, last_checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training SAMIL - MoCo Pretraining\n",
    "\n",
    "For the second contribution of bag-level pretraining, the paper describes a pretraining strategy that builds upon MoCo, a recent method for self-supervised image-level contrastive learning (img-CL). \n",
    "\n",
    "Self-superved learning (SSL) is a way to pre-train models that can be used for downstream tasks. The main idea is for the learned embedding of a training image to be similar to embeddings of slight transformations of the image while still being different from embeddings of other training images. The corresponding loss function is called InfoNCE where NCE stands for Noise-Contrastive Estimation, involving positive samples (similar samples) and negative samples (dissimilar samples). \n",
    "\n",
    "\"To obtain \"similar\" images, each image goes through different transformations to yield two versions of itself. These images are then encoded into an L-dimensional feature space by composing a projection layer... onto the output of the instance-level representation layer f\". Note that here, f represents the embedding of an image, not of the entire bag or study. To obtain \"dissimilar\" images, MoCo retrieves some previous embeddings, usually in the same training batch, which are treated as \"negative keys\".\n",
    "\n",
    "It is important to note that MoCo is a pre-training task, meaning the architecture/implementation of SAMIL is still the same as without pretraining. MoCo trains a model where there is a projection layer on top of the feature extraction layers of SAMIL. By training this model with this different objective, the feature extraction layers can be pre-trained with weights that generate a richer embedding with an ability to be similar to similar images (i.e. images from a study with the same label) and dissimilar from dissimilar images (i.e. images from a study with a different label). At the end of MoCo training, the projection layer can be discarded and the feature extraction layer weights extracted for use in the original SAMIL training task. It is like giving the SAMIL model a warm start to its original objective.\n",
    "\n",
    "Note that the original GitHub does not include the code for MoCo training, only the pretrained checkpoints, so for this ablation and the next (bag-level MoCo), we are only loading the pretrained weights and applying them to SAMIL before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading pretrained checkpoint either for MoCO instance-level or bag-level\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "def create_pretrained_SAMIL_model(checkpoint_path):\n",
    "    model = SAMIL()\n",
    "\n",
    "    if checkpoint_path != \"\":\n",
    "        print(\"Loading pretrained weights\")\n",
    "\n",
    "        pretrained_dict = torch.load(checkpoint_path)\n",
    "        new_state_dict = OrderedDict()\n",
    "\n",
    "        for k, v in pretrained_dict.items():\n",
    "            if \"encoder_q\" in k:\n",
    "                name = \".\".join(k.split(\".\")[1:])\n",
    "                new_state_dict[name] = v\n",
    "\n",
    "        model_dict = model.state_dict()\n",
    "\n",
    "        new_state_dict = {k: v for k, v in new_state_dict.items() if k in model_dict}\n",
    "        model_dict.update(new_state_dict)\n",
    "\n",
    "        model.load_state_dict(model_dict)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training SAMIL - img-CL\n",
    "\n",
    "Below is the main training loop. Do not run the cell if you do not want to train the model. Alternatively, ensure that `num_epochs` is something low like 2 if you want to verify the code works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters from original paper, these are the same for all SAMIL ablations\n",
    "class_weights = torch.tensor([0.463, 0.342, 0.195]).to(DEVICE)  # Class weights for CrossEntropyLoss from original GitHub\n",
    "weight_decay = 0.0001  # Weight decay from Appendix Table C.1 SAMIL Split 1\n",
    "learning_rate = 0.0005  # Learning rate from Appendix Table C.1 SAMIL Split 1\n",
    "\n",
    "temperature = 0.1  # Temperature from Appendix Table C.1 SAMIL Split 1\n",
    "lambda_view_regularization = 15.0  # Lambda_SA from Appendix Table C.1 SAMIL Split 1\n",
    "\n",
    "num_epochs = 2000  # All training is done with 2000 epochs with early stop from Appendix C\n",
    "# Also using cosine schedule for all training from Appendix C\n",
    "\n",
    "eval_freq_epochs = 1  # Frequency of evaluating model's balanced accuracy on val set\n",
    "\n",
    "train_csv = os.path.join(RUNS_DIR, \"SAMIL-imgcl\", \"train_last_checkpoint.csv\")\n",
    "val_csv = os.path.join(RUNS_DIR, \"SAMIL-imgcl\", \"val_last_checkpoint.csv\")\n",
    "checkpoint_filepath = os.path.join(RUNS_DIR, \"SAMIL-imgcl\", \"best_checkpoint.pth.tar\")\n",
    "best_checkpoint_filepath = os.path.join(RUNS_DIR, \"SAMIL-imgcl\", \"best_test_checkpoint.pth.tar\")\n",
    "last_checkpoint_filepath = os.path.join(RUNS_DIR, \"SAMIL-imgcl\", \"last_checkpoint.pth.tar\")\n",
    "\n",
    "train_loader = DataLoader(train_randaug_dataset, batch_size=1, shuffle=True, num_workers=8)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=8)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=8)\n",
    "\n",
    "pretrained_path = os.path.join(CHECKPOINTS_DIR, \"MOCO_Pretraining_ImageLevel\", \"seed0_checkpoint.pt\")\n",
    "SAMIL_imgcl_model = create_pretrained_SAMIL_model(pretrained_path).to(DEVICE)\n",
    "view_model = create_view_model().to(DEVICE)\n",
    "\n",
    "no_decay = {\"bias\", \"bn\"}\n",
    "\n",
    "grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [\n",
    "            p\n",
    "            for n, p in SAMIL_imgcl_model.named_parameters()\n",
    "            if not any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        \"weight_decay\": weight_decay,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [\n",
    "            p\n",
    "            for n, p in SAMIL_imgcl_model.named_parameters()\n",
    "            if any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "\n",
    "optimizer = optim.SGD(grouped_parameters, lr=learning_rate, momentum=0.9, nesterov=True)\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, 0, num_epochs)\n",
    "\n",
    "SAMIL_imgcl_model = train_SAMIL_model(\n",
    "    SAMIL_imgcl_model,\n",
    "    view_model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    test_loader,\n",
    "    optimizer,\n",
    "    train_csv,\n",
    "    val_csv,\n",
    "    checkpoint_filepath,\n",
    "    best_checkpoint_filepath,\n",
    "    class_weights,\n",
    "    num_epochs,\n",
    "    eval_freq_epochs,\n",
    "    temperature,\n",
    "    lambda_view_regularization,\n",
    "    scheduler=scheduler,\n",
    ")\n",
    "\n",
    "save_model_checkpoint(SAMIL_imgcl_model, last_checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training SAMIL - \"Bag-level\" Pretraining\n",
    "\n",
    "This ablation is the proposed SAMIL model with both contributions and should theoretically have the best performance of all ablations.\n",
    "\n",
    "MoCo is adapted to self-supervised learning at the bag-level rather than image/instance-level. The overall idea is the same, but the method is modified to find similar \"bags\" or studies rather than similar \"instances\" or images. InfoNCE loss is also correspondingly changed.\n",
    "\n",
    "Just like img-CL, the MoCo implementation and training code are not provided, and only the pretrained weights for the feature extraction layers of SAMIL are provided.\n",
    "\n",
    "Below is the main training loop. Do not run the cell if you do not want to train the model. Alternatively, ensure that `num_epochs` is something low like 2 if you want to verify the code works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters from original paper, these are the same for all SAMIL ablations\n",
    "class_weights = torch.tensor([0.463, 0.342, 0.195]).to(DEVICE)  # Class weights for CrossEntropyLoss from original GitHub\n",
    "learning_rate = 0.0005  # Learning rate from Appendix Table C.1. SAMIL Split 1\n",
    "weight_decay = 0.0001  # Weight decay from Appendix Table C.1 SAMIL Split 1\n",
    "\n",
    "temperature = 0.1  # Temperature from Appendix Table C.1 SAMIL Split 1\n",
    "lambda_view_regularization = 15.0  # Lambda_SA from Appendix Table C.1 SAMIL Split 1\n",
    "\n",
    "num_epochs = 2000  # All training is done with 2000 epochs with early stop from Appendix C\n",
    "# Also using cosine schedule for all training from Appendix C\n",
    "\n",
    "eval_freq_epochs = 1  # Frequency of evaluating model's balanced accuracy on val set\n",
    "\n",
    "train_csv = os.path.join(RUNS_DIR, \"SAMIL-bag-pretrain\", \"train_last_checkpoint.csv\")\n",
    "val_csv = os.path.join(RUNS_DIR, \"SAMIL-bag-pretrain\", \"val_last_checkpoint.csv\")\n",
    "checkpoint_filepath = os.path.join(RUNS_DIR, \"SAMIL-bag-pretrain\", \"best_checkpoint.pth.tar\")\n",
    "best_checkpoint_filepath = os.path.join(RUNS_DIR, \"SAMIL-bag-pretrain\", \"best_test_checkpoint.pth.tar\")\n",
    "last_checkpoint_filepath = os.path.join(RUNS_DIR, \"SAMIL-bag-pretrain\", \"last_checkpoint.pth.tar\")\n",
    "\n",
    "train_loader = DataLoader(train_randaug_dataset, batch_size=1, shuffle=True, num_workers=8)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=8)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=8)\n",
    "\n",
    "pretrained_path = os.path.join(\n",
    "    CHECKPOINTS_DIR, \"MOCO_Pretraining_StudyLevel\", \"seed0_checkpoint.pt\"\n",
    ")\n",
    "SAMIL_bag_pretrain_model = create_pretrained_SAMIL_model(pretrained_path).to(DEVICE)\n",
    "view_model = create_view_model().to(DEVICE)\n",
    "\n",
    "no_decay = {\"bias\", \"bn\"}\n",
    "\n",
    "grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [\n",
    "            p\n",
    "            for n, p in SAMIL_bag_pretrain_model.named_parameters()\n",
    "            if not any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        \"weight_decay\": weight_decay,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [\n",
    "            p\n",
    "            for n, p in SAMIL_bag_pretrain_model.named_parameters()\n",
    "            if any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "\n",
    "optimizer = optim.SGD(grouped_parameters, lr=learning_rate, momentum=0.9, nesterov=True)\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, 0, num_epochs)\n",
    "\n",
    "SAMIL_bag_pretrain_model = train_SAMIL_model(\n",
    "    SAMIL_bag_pretrain_model,\n",
    "    view_model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    test_loader,\n",
    "    optimizer,\n",
    "    train_csv,\n",
    "    val_csv,\n",
    "    checkpoint_filepath,\n",
    "    best_checkpoint_filepath,\n",
    "    class_weights,\n",
    "    num_epochs,\n",
    "    eval_freq_epochs,\n",
    "    temperature,\n",
    "    lambda_view_regularization,\n",
    "    scheduler=scheduler,\n",
    ")\n",
    "\n",
    "save_model_checkpoint(SAMIL_bag_pretrain_model, last_checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "We are using balanced accuracy as the performance metric for evaluation just like the original paper. The TMED-2 dataset is fairly imbalanced, favoring more studies with a severe AS label, meaning standard accuracy is not suitable.\n",
    "\n",
    "NOTE: Please see the section under Training -> Helper Functions -> Eval Function: Balanced Accuracy for the implementation code of the evaluation metric.\n",
    "\n",
    "The code in the sections below will load saved checkpoints of each model and evaluate balanced accuracy on the test set using the defined helper function above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing ABMIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_ABMIL_bal_acc = 0.585 # Original paper's balanced accuracy for Split 1 from Table 3 and 4\n",
    "\n",
    "# Testing by loading a saved checkpoint\n",
    "load_model_path = os.path.join(ARCHIVED_RUNS_DIR, \"ABMIL-52.5\", \"best_test_checkpoint.pth.tar\")\n",
    "\n",
    "loaded_ABMIL_model = Attention().to(DEVICE)\n",
    "loaded_ABMIL_model.load_state_dict(torch.load(load_model_path))\n",
    "\n",
    "loaded_ABMIL_model.eval()\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=8)\n",
    "bal_acc = eval_model(loaded_ABMIL_model, test_loader)\n",
    "\n",
    "print(f\"Paper's bal_acc: {paper_ABMIL_bal_acc}\")\n",
    "print(f\"Bal_acc: {bal_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing SAMIL with No Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_NoPretrain_SAMIL_bal_acc = 0.727\n",
    "\n",
    "# Testing by loading a saved checkpoint\n",
    "load_model_path = os.path.join(ARCHIVED_RUNS_DIR, \"SAMIL-70.2\", \"best_test_checkpoint.pth.tar\")\n",
    "\n",
    "loaded_SAMIL_Model = SAMIL().to(DEVICE)\n",
    "loaded_SAMIL_Model.load_state_dict(torch.load(load_model_path))\n",
    "\n",
    "loaded_SAMIL_Model.eval()\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=8)\n",
    "bal_acc = eval_model(loaded_SAMIL_Model, test_loader)\n",
    "\n",
    "print(f\"Paper's bal_acc: {paper_NoPretrain_SAMIL_bal_acc}\")\n",
    "print(f\"Bal_acc: {bal_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing SAMIL with img-CL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_ImgCL_SAMIL_bal_acc = 0.712\n",
    "\n",
    "# Testing by loading a saved checkpoint\n",
    "load_model_path = os.path.join(ARCHIVED_RUNS_DIR, \"SAMIL-imgcl-70.3\", \"best_test_checkpoint.pth.tar\")\n",
    "\n",
    "loaded_SAMIL_Model = SAMIL().to(DEVICE)\n",
    "loaded_SAMIL_Model.load_state_dict(torch.load(load_model_path))\n",
    "\n",
    "loaded_SAMIL_Model.eval()\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=8)\n",
    "bal_acc = eval_model(loaded_SAMIL_Model, test_loader)\n",
    "\n",
    "print(f\"Paper's bal_acc: {paper_ImgCL_SAMIL_bal_acc}\")\n",
    "print(f\"Bal_acc: {bal_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing SAMIL with Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_Pretrain_SAMIL_bal_acc = 0.754\n",
    "\n",
    "# Testing by loading a saved checkpoint\n",
    "load_model_path = os.path.join(ARCHIVED_RUNS_DIR, \"SAMIL-bag-pretrain-72.9\", \"best_test_checkpoint.pth.tar\")\n",
    "\n",
    "loaded_SAMIL_Model = SAMIL().to(torch.device(\"cuda\"))\n",
    "loaded_SAMIL_Model.load_state_dict(torch.load(load_model_path))\n",
    "\n",
    "loaded_SAMIL_Model.eval()\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=8)\n",
    "bal_acc = eval_model(loaded_SAMIL_Model, test_loader)\n",
    "\n",
    "print(f\"Paper's bal_acc: {paper_Pretrain_SAMIL_bal_acc}\")\n",
    "print(f\"Bal_acc: {bal_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results Table Comparing to Original Paper's Results\n",
    "\n",
    "| Ablation           | Reported Bal. Acc. | Our Bal. Acc. |\n",
    "| ------------------ | ------------------ | ------------- |\n",
    "| ABMIL              | 58.5               | 52.5          |\n",
    "| SAMIL no pretrain  | 72.7               | 70.2          |\n",
    "| SAMIL img-CL       | 71.2               | 70.3          |\n",
    "| SAMIL bag pretrain | 75.4               | 72.9          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comparing Train Loss over Epochs for each Ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ABMIL_df = pd.read_csv(os.path.join(ARCHIVED_RUNS_DIR, \"ABMIL-52.5\", \"train_last_checkpoint.csv\"))\n",
    "SAMIL_df = pd.read_csv(os.path.join(ARCHIVED_RUNS_DIR, \"SAMIL-70.2\", \"train_last_checkpoint.csv\"))\n",
    "SAMIL_imgcl_df = pd.read_csv(os.path.join(ARCHIVED_RUNS_DIR, \"SAMIL-imgcl-70.3\", \"train_last_checkpoint.csv\"))\n",
    "SAMIL_bag_df = pd.read_csv(os.path.join(ARCHIVED_RUNS_DIR, \"SAMIL-bag-pretrain-72.9\", \"train_last_checkpoint.csv\"))\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(ABMIL_df[\"epoch\"], ABMIL_df[\"loss\"], color=\"tab:blue\", label=\"ABMIL\")\n",
    "ax.plot(SAMIL_df[\"epoch\"], SAMIL_df[\"loss\"], color=\"tab:orange\", label=\"SAMIL\")\n",
    "ax.plot(SAMIL_imgcl_df[\"epoch\"], SAMIL_imgcl_df[\"loss\"], color=\"tab:purple\", label=\"SAMIL-imgcl\")\n",
    "ax.plot(SAMIL_bag_df[\"epoch\"], SAMIL_bag_df[\"loss\"], color=\"tab:green\", label=\"SAMIL-Bag\")\n",
    "\n",
    "plt.xlabel(\"Num Epochs\")\n",
    "plt.ylabel(\"Mean Loss\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comparing Balanced Accuracy over Epochs for each Ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import Axes\n",
    "\n",
    "def draw_trend_line(ax: Axes, csv_path: str, x_col: str, y_col: str, color: str, label: str, curve_degree: int):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df = df[pd.notna(df[y_col])]\n",
    "\n",
    "    z = np.polyfit(df[x_col], df[y_col], curve_degree)\n",
    "    p = np.poly1d(z)\n",
    "\n",
    "    ax.plot(df[x_col], p(df[x_col]), color=color, label=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ABMIL_path = os.path.join(ARCHIVED_RUNS_DIR, \"ABMIL-52.5\", \"val_last_checkpoint.csv\")\n",
    "SAMIL_path = os.path.join(ARCHIVED_RUNS_DIR, \"SAMIL-70.2\", \"val_last_checkpoint.csv\")\n",
    "SAMIL_imgcl_path = os.path.join(ARCHIVED_RUNS_DIR, \"SAMIL-imgcl-70.3\", \"val_last_checkpoint.csv\")\n",
    "SAMIL_bag_path = os.path.join(ARCHIVED_RUNS_DIR, \"SAMIL-bag-pretrain-72.9\", \"val_last_checkpoint.csv\")\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "fig.set_figwidth(15)\n",
    "# ax = fig.add_subplot(1, 1, 1)\n",
    "draw_trend_line(ax2, ABMIL_path, \"epoch\", \"val_bal_acc\", \"blue\", \"ABMIL\", 16)\n",
    "draw_trend_line(ax2, SAMIL_path, \"epoch\", \"val_bal_acc\", \"orange\", \"SAMIL\", 16)\n",
    "draw_trend_line(ax2, SAMIL_imgcl_path, \"epoch\", \"val_bal_acc\", \"purple\", \"SAMIL-imgcl\", 16)\n",
    "draw_trend_line(ax2, SAMIL_bag_path, \"epoch\", \"val_bal_acc\", \"green\", \"SAMIL-bag\", 16)\n",
    "\n",
    "ax1.plot(\"epoch\", \"val_bal_acc\", data=pd.read_csv(ABMIL_path), color=\"blue\", label=\"ABMIL\")\n",
    "ax1.plot(\"epoch\", \"val_bal_acc\", data=pd.read_csv(SAMIL_path), color=\"orange\", label=\"SAMIL\")\n",
    "ax1.plot(\"epoch\", \"val_bal_acc\", data=pd.read_csv(SAMIL_imgcl_path), color=\"purple\", label=\"SAMIL-imgcl\")\n",
    "ax1.plot(\"epoch\", \"val_bal_acc\", data=pd.read_csv(SAMIL_bag_path), color=\"green\", label=\"SAMIL-Bag\")\n",
    "\n",
    "ax1.set_xlabel(\"Num Epochs\")\n",
    "ax2.set_xlabel(\"Num Epochs\")\n",
    "plt.ylabel(\"Bal Acc on Val Set\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plans"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SAMIL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
